{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d7a97c",
   "metadata": {},
   "source": [
    "In the context of **Large Language Models (LLMs)** like ChatGPT, **system prompts** and **prompting techniques** are critical concepts. Here's a full explanation, including definitions and the various prompting types like **one-shot, few-shot**, etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ What is a **System Prompt**?\n",
    "\n",
    "A **system prompt** is a special type of instruction given to guide the behavior, tone, or identity of the LLM before any user interaction. It‚Äôs **not visible** to the user (in most APIs), but it helps the model stay aligned with the desired behavior.\n",
    "\n",
    "##### Example of a system prompt:\n",
    "\n",
    "> ‚ÄúYou are a helpful assistant that explains complex ideas in simple terms.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "#### üìö Prompting Techniques for LLMs\n",
    "\n",
    "These refer to how input examples, instructions, or tasks are structured when interacting with the LLM. There are several key types:\n",
    "\n",
    "---\n",
    "\n",
    "##### 1. **Zero-shot Prompting**\n",
    "\n",
    "* **Definition**: No examples are provided. You simply ask the model to perform a task.\n",
    "* **Use Case**: When you trust the model to generalize from instruction alone.\n",
    "* **Example**:\n",
    "\n",
    "  ```\n",
    "  Translate this sentence to French: \"I love programming.\"\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "##### 2. **One-shot Prompting**\n",
    "\n",
    "* **Definition**: One example is given before the task.\n",
    "* **Use Case**: When you want to guide the model with a single example.\n",
    "* **Example**:\n",
    "\n",
    "  ```\n",
    "  Q: What is the capital of France?\n",
    "  A: Paris\n",
    "\n",
    "  Q: What is the capital of Japan?\n",
    "  A:\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "##### 3. **Few-shot Prompting**\n",
    "\n",
    "* **Definition**: Multiple examples are given to teach the model the desired behavior or format.\n",
    "* **Use Case**: Improves performance on complex tasks by showing patterns.\n",
    "* **Example**:\n",
    "\n",
    "  ```\n",
    "  Q: What is the capital of Germany?\n",
    "  A: Berlin\n",
    "\n",
    "  Q: What is the capital of Italy?\n",
    "  A: Rome\n",
    "\n",
    "  Q: What is the capital of India?\n",
    "  A:\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "##### 4. **Chain-of-Thought (CoT) Prompting**\n",
    "\n",
    "* **Definition**: Encourage the model to explain its reasoning step-by-step.\n",
    "* **Use Case**: Complex reasoning tasks like math or logic.\n",
    "* **Example**:\n",
    "\n",
    "  ```\n",
    "  Q: If there are 5 apples and you eat 2, how many are left?\n",
    "  A: There were 5 apples. I ate 2. So, 5 - 2 = 3 apples are left.\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "##### 5. **ReAct Prompting (Reasoning + Acting)**\n",
    "\n",
    "* **Definition**: Combines reasoning and actions (often used with tools/agents).\n",
    "* **Use Case**: Interactive agents, tool-using LLMs.\n",
    "* **Example** (simplified):\n",
    "\n",
    "  ```\n",
    "  Thought: I need to search Wikipedia to answer this.\n",
    "  Action: Search[‚ÄúEinstein Nobel Prize‚Äù]\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "##### 6. **Self-consistency Prompting**\n",
    "\n",
    "* **Definition**: Run the same prompt multiple times, aggregate the results (used with CoT).\n",
    "* **Use Case**: Improve accuracy in tasks with uncertain outputs.\n",
    "* **How**: Ask multiple CoT completions, then take the majority answer.\n",
    "\n",
    "---\n",
    "\n",
    "##### 7. **Instruction Tuning**\n",
    "\n",
    "* **Definition**: LLMs are fine-tuned on datasets of instructions and responses.\n",
    "* **Use Case**: Most modern models like ChatGPT are instruction-tuned.\n",
    "* **Example** (like zero-shot, but model was trained this way):\n",
    "\n",
    "  ```\n",
    "  Summarize the following paragraph.\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "##### 8. **Soft Prompting / Prompt Tuning**\n",
    "\n",
    "* **Definition**: Instead of text prompts, use learned embeddings as prompts.\n",
    "* **Use Case**: Needs model internals; used in fine-tuning setups.\n",
    "\n",
    "---\n",
    "\n",
    "##### 9. **Auto Prompting / Prompt Engineering Tools**\n",
    "\n",
    "* **Definition**: Use automated tools or algorithms to generate effective prompts.\n",
    "* **Use Case**: Boost performance without manual prompt crafting.\n",
    "\n",
    "---\n",
    "\n",
    "##### 10. **Contextual Prompting**\n",
    "\n",
    "* **Definition**: Inject context (e.g., user preferences, previous dialogue) into the prompt.\n",
    "* **Use Case**: Conversational agents, personalized assistants.\n",
    "* **Example**:\n",
    "\n",
    "  ```\n",
    "  User previously mentioned liking sci-fi. Recommend a movie based on that.\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "##### üîß Advanced Techniques\n",
    "\n",
    "* **Meta-Prompting**: Prompting the model to generate prompts.\n",
    "* **Prompt Chaining**: Outputs of one prompt become input to the next.\n",
    "* **Retrieval-Augmented Prompting**: Retrieve documents and include them in the prompt (e.g., RAG).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f6855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "491fbdd5",
   "metadata": {},
   "source": [
    "Great! Here's a **complete guide with real examples and templates** for each **prompting technique**, focusing on **OpenAI API** and **HuggingFace Transformers** usage. You can adapt them to any LLM system like Groq, Mistral, Llama, etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîß 1. Zero-shot Prompting\n",
    "\n",
    "##### üîπ Prompt:\n",
    "\n",
    "```python\n",
    "prompt = \"Summarize the following text:\\n\\nClimate change is causing glaciers to melt at an unprecedented rate...\"\n",
    "```\n",
    "\n",
    "##### üîπ OpenAI Example:\n",
    "\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### üîß 2. One-shot Prompting\n",
    "\n",
    "##### üîπ Prompt:\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"Translate the sentence to French.\n",
    "English: I love apples.\n",
    "French: J'aime les pommes.\n",
    "\n",
    "English: I am going to the market.\n",
    "French:\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### üîß 3. Few-shot Prompting\n",
    "\n",
    "##### üîπ Prompt:\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"Convert numbers to words:\n",
    "Input: 5\n",
    "Output: five\n",
    "\n",
    "Input: 23\n",
    "Output: twenty-three\n",
    "\n",
    "Input: 104\n",
    "Output:\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### üîß 4. Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "##### üîπ Prompt:\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"Q: There are 3 red balls and 5 blue balls. If I take 2 red balls, how many are left?\n",
    "A: There were 3 red balls. I took 2. So 3 - 2 = 1 red ball left.\n",
    "\n",
    "Q: John has 10 apples. He gives 4 to Mary and 2 to Tom. How many apples does John have now?\n",
    "A:\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üîß 5. ReAct Prompting (Reason + Action) \\[for Tool-using agents]\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"Question: What is the capital of the country where the Eiffel Tower is located?\n",
    "Thought: The Eiffel Tower is in Paris, which is in France.\n",
    "Action: Answer[Paris]\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Used in **LangChain** or **agent-based tool models**.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîß 6. Self-consistency Prompting\n",
    "\n",
    "##### üîπ Strategy:\n",
    "\n",
    "Run the same **Chain-of-Thought prompt** multiple times and **take the most common answer**.\n",
    "\n",
    "##### üîπ Python logic:\n",
    "\n",
    "```python\n",
    "answers = []\n",
    "for _ in range(5):\n",
    "    res = ask_model_with_cot(prompt)\n",
    "    answers.append(res)\n",
    "\n",
    "final_answer = max(set(answers), key=answers.count)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### üîß 7. Instruction Tuning (like OpenAI GPT-4, Claude, Mistral)\n",
    "\n",
    "You just give clear instructions:\n",
    "\n",
    "```python\n",
    "prompt = \"List 5 healthy foods that are good for people with high blood pressure.\"\n",
    "```\n",
    "\n",
    "‚Üí Works best with **instruction-tuned models** like `gpt-4`, `mistral-instruct`, or `flan-t5`.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîß 8. Soft Prompting / Prompt Tuning (Advanced, Training Required)\n",
    "\n",
    "This uses **learned embeddings** as prompts ‚Äî no textual input. It's not applicable in APIs directly but used during model fine-tuning.\n",
    "\n",
    "üõ† Tools:\n",
    "\n",
    "* [PEFT](https://github.com/huggingface/peft) from HuggingFace\n",
    "* LoRA + Prompt Tuning combo\n",
    "\n",
    "---\n",
    "\n",
    "##### üîß 9. Contextual Prompting (Chat History or State)\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a movie assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I like thrillers and sci-fi.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you recommend a movie?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "LLMs will infer preferences from **earlier context**.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîß 10. Retrieval-Augmented Prompting (RAG)\n",
    "\n",
    "Used when LLMs need **external knowledge** from documents or DBs.\n",
    "\n",
    "##### üîπ Flow:\n",
    "\n",
    "1. User asks a question.\n",
    "2. Vector DB (like FAISS) retrieves relevant documents.\n",
    "3. You add retrieved content to the prompt.\n",
    "\n",
    "```python\n",
    "retrieved_context = \"Article: Glaciers are melting rapidly due to rising temperatures.\"\n",
    "prompt = f\"Based on the article, answer: Why are glaciers melting?\\n\\n{retrieved_context}\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† Bonus: Meta-Prompting (Prompt Generator)\n",
    "\n",
    "Ask LLM to **create a prompt** for you:\n",
    "\n",
    "```python\n",
    "prompt = \"Write a prompt to summarize any article in 3 bullet points.\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üß© Prompt Templates Summary Table\n",
    "\n",
    "| Prompt Type          | Example Use Case              | Needs Examples? | API Friendly  |\n",
    "| -------------------- | ----------------------------- | --------------- | ------------- |\n",
    "| Zero-shot            | Simple tasks                  | ‚ùå               | ‚úÖ             |\n",
    "| One-shot             | Slight guidance needed        | ‚úÖ (1)           | ‚úÖ             |\n",
    "| Few-shot             | Learn from patterns           | ‚úÖ (2-5)         | ‚úÖ             |\n",
    "| Chain-of-Thought     | Step-by-step logic            | Optional        | ‚úÖ             |\n",
    "| ReAct                | Tool use + reasoning          | ‚úÖ               | ‚ö†Ô∏è (advanced) |\n",
    "| Self-consistency     | Improve CoT accuracy          | ‚úÖ               | ‚úÖ             |\n",
    "| Instruction Tuning   | Just use instructions         | ‚ùå               | ‚úÖ             |\n",
    "| Soft Prompting       | Fine-tuning context injection | ‚ùå (embedding)   | ‚ùå (training)  |\n",
    "| Contextual Prompting | Chatbots, memory              | ‚ùå               | ‚úÖ             |\n",
    "| Retrieval-Augmented  | Custom knowledge (RAG)        | ‚úÖ               | ‚úÖ             |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe7575",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4df85384",
   "metadata": {},
   "source": [
    " **advanced prompting techniques** with explanations **+ real-world examples + ready-to-use Python templates**. After this, I can bundle all of them into a downloadable notebook/script set if you want.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÅ 1. **Prompt Chaining**\n",
    "\n",
    "##### üîπ What It Is:\n",
    "\n",
    "The **output of one prompt** becomes the **input for the next** ‚Äî like a workflow. This is used for **multi-step reasoning**, **data extraction ‚Üí summarization**, or **query ‚Üí search ‚Üí answer** pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úÖ Example: Step-by-step Chain in Python\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"your-api-key\")\n",
    "\n",
    "# Step 1: Extract keywords from a query\n",
    "query = \"Tell me about the environmental impact of electric vehicles.\"\n",
    "step1 = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"Extract keywords: {query}\"}]\n",
    ").choices[0].message.content\n",
    "\n",
    "# Step 2: Use extracted keywords to generate a search query\n",
    "step2 = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"Formulate a research question based on: {step1}\"}]\n",
    ").choices[0].message.content\n",
    "\n",
    "# Step 3: Generate a short paragraph answer\n",
    "final_output = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"Write a 150-word summary on: {step2}\"}]\n",
    ").choices[0].message.content\n",
    "\n",
    "print(final_output)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† 2. **Meta-Prompting (Prompt Generator)**\n",
    "\n",
    "##### üîπ What It Is:\n",
    "\n",
    "You ask the model to **generate prompts** that can then be used for another task. Useful for **autonomous agents**, **prompt optimization**, or **dynamic LLM pipelines**.\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úÖ Example: Generating a custom prompt\n",
    "\n",
    "```python\n",
    "meta_prompt = \"\"\"You are a prompt engineer. Create a prompt that asks a language model to generate 3 bullet points summarizing any article.\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": meta_prompt}]\n",
    ")\n",
    "\n",
    "print(\"Generated Prompt:\", response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "üîÅ Then, use the generated prompt as input for another LLM call.\n",
    "\n",
    "---\n",
    "\n",
    "##### üß† Use Case: Self-Improving Chatbot\n",
    "\n",
    "1. User input ‚Üí LLM generates the optimal prompt.\n",
    "2. That prompt is passed to another model to answer.\n",
    "3. Result is refined using another call.\n",
    "\n",
    "You can build **auto agents** like BabyAGI or AutoGPT with this pattern.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîé 3. **Retrieval-Augmented Generation (RAG)**\n",
    "\n",
    "##### üîπ What It Is:\n",
    "\n",
    "RAG lets you **inject knowledge** into LLMs by retrieving relevant documents and adding them to the prompt. Great for **chat with PDFs**, **custom databases**, or **domain-specific QA**.\n",
    "\n",
    "---\n",
    "\n",
    "##### ‚úÖ Minimal Working RAG Example (FAISS + OpenAI)\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "# Setup\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "client = OpenAI(api_key=\"your-api-key\")\n",
    "docs = [\n",
    "    \"Electric vehicles reduce CO2 emissions significantly.\",\n",
    "    \"EV batteries are made from lithium and cobalt.\",\n",
    "    \"Charging infrastructure is critical for EV adoption.\"\n",
    "]\n",
    "doc_embeddings = model.encode(docs, convert_to_numpy=True)\n",
    "index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# User query\n",
    "query = \"How do electric vehicles help the environment?\"\n",
    "query_vec = model.encode([query], convert_to_numpy=True)\n",
    "_, indices = index.search(query_vec, k=2)\n",
    "\n",
    "# Create prompt\n",
    "relevant_context = \"\\n\".join([docs[i] for i in indices[0]])\n",
    "rag_prompt = f\"Use the following information to answer:\\n\\n{relevant_context}\\n\\nQ: {query}\\nA:\"\n",
    "\n",
    "# Generate answer\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": rag_prompt}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üß∞ Would You Like a Downloadable Notebook?\n",
    "\n",
    "I'll prepare a **single script or Jupyter notebook** that includes:\n",
    "\n",
    "* ‚úÖ Zero, One, Few-shot prompting\n",
    "* ‚úÖ CoT + Self-consistency\n",
    "* ‚úÖ RAG with FAISS\n",
    "* ‚úÖ ReAct agent with LangChain\n",
    "* ‚úÖ Prompt chaining pipeline\n",
    "* ‚úÖ Meta-prompt generation\n",
    "\n",
    "Let me know your preference:\n",
    "\n",
    "* üìÑ Python script (`.py`)\n",
    "* üìò Jupyter Notebook (`.ipynb`)\n",
    "* üí° Or both?\n",
    "\n",
    "Also, do you want it to be **OpenAI-only**, or should I include **HuggingFace + Groq-compatible variants** too?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32501c02",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7428ab17",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e4603e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467bfa70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26dfe4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66cc9236",
   "metadata": {},
   "source": [
    "``` from pathlib import Path ```\n",
    "\n",
    "# Create Jupyter Notebook (.ipynb) with multiple LLM prompting examples\n",
    "``` from nbformat import v4 as nbf\n",
    "\n",
    "notebook = nbf.new_notebook()\n",
    "cells = []\n",
    "\n",
    " ```\n",
    "\n",
    "# Title and Intro\n",
    "``` cells.append(nbf.new_markdown_cell(\"# üí° LLM Prompting Techniques (Hands-on Examples)\\n\\nThis notebook includes examples of core and advanced prompting methods using OpenAI, HuggingFace, and FAISS.\"))\n",
    "``` \n",
    "# 1. Few-shot Prompting\n",
    "few_shot_code = '''\\\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load instruction-tuned model\n",
    "``` generator = pipeline(\"text-generation\", model=\"google/flan-t5-base\")\n",
    "``` \n",
    "prompt = \"\"\"\n",
    "Q: What is the capital of France?\n",
    "A: Paris\n",
    "\n",
    "Q: What is the capital of Germany?\n",
    "A: Berlin\n",
    "\n",
    "Q: What is the capital of Japan?\n",
    "A:\"\"\"\n",
    "\n",
    "``` response = generator(prompt, max_new_tokens=20, do_sample=False)\n",
    "print(response[0]['generated_text'])\n",
    "'''\n",
    "cells.append(nbf.new_markdown_cell(\"## üß™ Few-shot Prompting (HuggingFace)\"))\n",
    "cells.append(nbf.new_code_cell(few_shot_code))\n",
    "``` \n",
    "# 2. RAG with FAISS\n",
    "``` rag_code = '''\\\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "import numpy as np\n",
    "``` \n",
    "# Load model & documents\n",
    "``` model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "documents = [\n",
    "    \"Glaciers are melting due to climate change.\",\n",
    "    \"Apples are good for heart health.\",\n",
    "    \"Python is a programming language.\"\n",
    "]\n",
    "``` \n",
    "# Embed docs and create FAISS index\n",
    "``` doc_embeddings = model.encode(documents, convert_to_numpy=True)\n",
    "index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
    "index.add(doc_embeddings)\n",
    "``` \n",
    "# User query\n",
    "``` query = \"Why are glaciers disappearing?\"\n",
    "query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "_, I = index.search(query_embedding, k=1)\n",
    "matched_text = documents[I[0][0]]\n",
    "``` \n",
    "# Prompt\n",
    "```\n",
    "prompt = f\"Answer the question based on the following:\\n\\n{matched_text}\\n\\nQ: {query}\\nA:\"\n",
    "print(prompt)\n",
    "'''\n",
    "cells.append(nbf.new_markdown_cell(\"## üîç Retrieval-Augmented Generation (RAG) with FAISS\"))\n",
    "cells.append(nbf.new_code_cell(rag_code))\n",
    "``` \n",
    "# 3. Prompt Chaining\n",
    "``` chain_code = '''\\\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"your-api-key\")\n",
    "``` \n",
    "# Step 1: Extract keywords\n",
    "``` query = \"Tell me about the environmental impact of electric vehicles.\"\n",
    "step1 = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"Extract keywords: {query}\"}]\n",
    ").choices[0].message.content\n",
    "``` \n",
    "# Step 2: Generate research question\n",
    "``` step2 = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"Create a question from: {step1}\"}]\n",
    ").choices[0].message.content\n",
    "``` \n",
    "# Step 3: Final answer\n",
    "``` final_output = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"Write a 150-word answer about: {step2}\"}]\n",
    ").choices[0].message.content\n",
    "\n",
    "print(final_output)\n",
    "'''\n",
    "cells.append(nbf.new_markdown_cell(\"## üîÅ Prompt Chaining with OpenAI\"))\n",
    "cells.append(nbf.new_code_cell(chain_code))\n",
    "``` \n",
    "# 4. Meta-Prompting\n",
    "``` meta_prompt_code = '''\\\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"your-api-key\")\n",
    "\n",
    "meta_prompt = \"You are a prompt engineer. Write a prompt that asks an LLM to summarize any article in 3 bullet points.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": meta_prompt}]\n",
    ")\n",
    "\n",
    "print(\"Generated Prompt:\", response.choices[0].message.content)\n",
    "'''\n",
    "cells.append(nbf.new_markdown_cell(\"## üß† Meta-Prompting (Prompt Generator)\"))\n",
    "cells.append(nbf.new_code_cell(meta_prompt_code))\n",
    "``` \n",
    "# 5. Chain-of-Thought Prompting\n",
    "``` cot_code = '''\\\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"your-api-key\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful reasoning assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Q: A train travels 60 km in 2 hours. How fast is it going?\\\\nA: Let's think step by step.\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=messages,\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "'''\n",
    "cells.append(nbf.new_markdown_cell(\"## üß© Chain-of-Thought Prompting\"))\n",
    "cells.append(nbf.new_code_cell(cot_code))\n",
    "``` \n",
    "# Finalize notebook\n",
    "``` notebook['cells'] = cells\n",
    "path = Path(\"/mnt/data/LLM_Prompting_Tutorial.ipynb\")\n",
    "path.write_text(nbf.writes(notebook))\n",
    "``` "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
